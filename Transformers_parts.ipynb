{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMvb3Ha9V8a93poeTlOD0O9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"HdNo4G-rqM5q","executionInfo":{"status":"ok","timestamp":1714848176141,"user_tz":-180,"elapsed":7125,"user":{"displayName":"Alexey Akulov","userId":"01094635187180112136"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","import os\n","import random\n","import math\n","\n","\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, max_seq_length):\n","        super(PositionalEncoding, self).__init__()\n","\n","        assert d_model % 2 == 0, 'Размерность эмбедингов должна быть чётной'\n","\n","        # Инициализируем матрицу нулей (max_seq_length, d_model), в которой будут храниться значения позиционных эмбедингов\n","        pe = torch.zeros(max_seq_length, d_model)\n","\n","        # Создайте переменную position (), равную длине последовательности и содержащую соответствующие значения (~1 строка кода)\n","        position = torch.arange(max_seq_length, dtype=torch.float).unsqueeze(1)\n","\n","        # Переменная div_term содержит стабильную версию вычисления 1/(10000)**(2i/d_model), то есть всего того, что находится\n","        # в числителе для чётной и нечётной позиции эмбединга.\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n","\n","        # Используйте переменную div_term для вычисления (~2 строки кода)\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","\n","        pe = pe.unsqueeze(0)\n","\n","        self.register_buffer('pe', pe) # добавляем доп размерность и указываем что эти параметры\n","                                                    # не буду оубчающимися\n","\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Вычисляет сумму входных и позиционных эмбедингов\n","\n","        Arguments:\n","            x (batch_size, position, d_model) -- Maximum number of positions to be encoded\n","\n","        Returns:\n","            x -- (batch_size, position, d_model) Матрицу\n","        \"\"\"\n","        # Вычислите сумму входных эмбедингов и позиционного кодирования (~1 строка кода)\n","        x = x + self.pe[:, :x.size(1)]\n","        return x"]}]}